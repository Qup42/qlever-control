#!/bin/bash

# This is the script that is called when calling "qlever" from the command line.
# It essentially calls "make", using the "Qleverfile" in the QLever home and in
# the current directory as Makefiles.
#
# Â© 2022, University of Freiburg, Chair of Algorithms and Data Structures
# Author: Hannah Bast <bast@cs.uni-freiburg.de>

BLUE=$(printf "\033[34m")
RED=$(printf "\033[31m")
GRAY=$(printf "\033[37m")
BOLD_ON=$(printf "\033[1m")
BOLD_OFF=$(printf "\033[22m")
NORMAL=$(printf "\033[0m")

# This command (possibly with relative or absolute path).
QLEVER_CMD="${BASH_SOURCE[0]}"

# Command to get the master Qleverfile (everything after __DATA__ in this file,
# see below).
GET_MASTER_QLEVERFILE="sed -e '1,/^__DATA__/d' ${QLEVER_CMD}"

# Command to leave the script ("return" when the script is sourced, "exit" when
# it is called normally, that is, within its own subshell).
if [ ${QLEVER_CMD} == $0 ]; then EXIT=exit; else EXIT=return; fi

# # Get the location of this script, from wherever it's called. If $QLEVER_HOME is
# # set, assume it's in $QLEVER_HOME/qlever-code. Otherwise, use $0.
# if [ ! -z ${QLEVER_HOME} ]; then
#   QLEVER_DIR=${QLEVER_HOME}/qlever-code
# else
#   QLEVER_DIR=$(dirname -- "$0")
# fi
# # Make sure we have the absolute path (by going to the directory in a subshell
# # and then simply doing pwd, see https://tinyurl.com/4jsurr86 ).
# QLEVER_DIR=$(cd -- "${QLEVER_DIR}" &> /dev/null && pwd)
# 
# # Directory with the data (index and configuration) of a particular QLever
# # application. Default: the directory from which the script is called.
APP_DIR=$(pwd)

echo
echo "${GRAY}Called \"${QLEVER_CMD}\" from \"${APP_DIR}\"${NORMAL}"
echo

# With arguments "reset", remove completions and local Qleverfile. TODO: warn
# before actually doing this because the Qleverfile might contain valuable
# informationC, so for now we are moving it.
if [ "${QLEVER_CMD}" != "$0" ] && [ ! -z "$*" ] && [ "$*" == "reset" ]; then
  complete -r qlever 2> /dev/null
  rm -f qlever.completions
  rm -f *.pid
  if [ -e Qleverfile ]; then
    QLEVERFILE_BACKUP=Qleverfile.DELETED_$(date +%Y%m%dT%H%M)
    echo "Moving Qleverfile to ${QLEVERFILE_BACKUP}"
    echo
    mv Qleverfile ${QLEVERFILE_BACKUP}
  fi
  echo "Reset configuration, you can now start from scratch."
  echo
  eval ${EXIT}
fi

# If there is COMPLETIONS_FILE yet, create it. For that we have to first ask the
# user to call the script with "source" or "." (it's the only way to activate
# bash completion).
COMPLETIONS_FILE=qlever.completions
if [ ! -e ${COMPLETIONS_FILE} ]; then
  # echo "\${BASH_SOURCE[0]}: ${BASH_SOURCE[0]}"
  # echo "\$0=$0"
  if [ ${BASH_SOURCE[0]} == $0 ]; then
    cat << EOT
It seems you are running this script for the first time in this shell and
directory. Please run it once as follows:

${BLUE}. qlever${NORMAL}

Afterwards you can just call "qlever" again (without the dot), and you will have
bash autocompletion available. If you don't care about bash autocompletion or
something doesn't work, just create an empty file ${COMPLETIONS_FILE} before
your proceed.

EOT
    eval ${EXIT}
  else
    # CASE 1.2: This script is being sourced -> activate bash completions and
    # write the completions to COMPLETIONS_FILE.
    COMPLETIONS=$(eval ${GET_MASTER_QLEVERFILE} | \grep -oE "^[a-zA-Z_]+:" | sed 's/:$//' | paste -sd" ")
    echo "${COMPLETIONS}" > ${COMPLETIONS_FILE}
    complete -W "${COMPLETIONS}" qlever
    cat << EOT
Activated bash autocompletion. The following completions will be available:

${BLUE}$(fold -s  ${COMPLETIONS_FILE})${NORMAL}

EOT
  fi
fi

# If the is no Qleverfile in this directory yet, create a basic one.
if [ ! -e Qleverfile ]; then
  # If a file with suffix .ttl or .nt exists, use the basename as the basename
  # for the index files (the DB variable in the configuration). If several such
  # files exist, take the largest one. If no such file exists, leave the
  # basename empty and for the user to set.
  INPUT_FILE=$(ls -S | egrep "\.(ttl|nt)(\.(gz|bz|bz2|xz))?\$" | head -1)
  if [ ! -z ${INPUT_FILE} ]; then DB=${INPUT_FILE/.*/}; else DB=; fi
    # Get right version of "cat", depending on suffix.
    CAT=cat
    if [[ ${INPUT_FILE} =~ \.gz\$ ]]; then CAT=zcat; fi
    if [[ ${INPUT_FILE} =~ \.bz\$ ]]; then CAT=bzcat; fi
    if [[ ${INPUT_FILE} =~ \.bz2\$ ]]; then CAT=bzca; fi
    if [[ ${INPUT_FILE} =~ \.xz\$ ]]; then CAT=xzca; fi
    cat << EOT > Qleverfile
# Qleverfile for folder $(pwd) .
# Created on $(date).
# Modify or expand as you see fit.

DB=${DB}
CAT_TTL=${CAT} ${INPUT_FILE}
PORT=7001
EOT
    cat << EOT
There was no Qleverfile in this directory yet. A Qleverfile contains basic
configuration telling the "qlever" command how to do certain things. I now
created the following basic Qleverfile.

${BLUE}$(cat Qleverfile)${NORMAL}

EOT
fi

# If the script was sourced (initial setup), exit (will be "return") now.
if [ ${QLEVER_CMD} != $0 ]; then
  cat << EOT | fold -s
Setup completed, you can now use "qlever" normally.

EOT
  eval ${EXIT}
fi

# When calling script with no arguments, show the config. TODO: Give the user
# some more information here.
if [ -z "$*" ]; then
  eval ${GET_MASTER_QLEVERFILE} | make -f - -f ${APP_DIR}/Qleverfile
  if [ -e ${COMPLETIONS_FILE} ]; then
    cat << EOT
All completions:

${BLUE}$(cat ${COMPLETIONS_FILE} | fold -s)${NORMAL}

To see what a particular command would do (without executing it), just type
something like "qlever index show" (single command) or "qlever start log show"
(multiple commands)

EOT
  fi
  eval ${EXIT}
fi

# If the last argument is "show", just show the commands, but don't execute
# them. The first for loop is a portable hack to get the last argument.
for LAST; do true; done
if [ "${LAST}" == "show" ]; then
  echo "${BOLD_ON}Just showing what would be executed${BOLD_OFF}"
  echo
  for ARG in "$@"; do
    if [ "${ARG}" != "show" ]; then
      echo "\"${ARG}\" would execute the following:"
      echo ${BLUE}
      eval ${GET_MASTER_QLEVERFILE} \
        | make -n -f - -f ${APP_DIR}/Qleverfile "${ARG}" | grep -v "^echo"
      echo ${NORMAL}
    fi
  done
  eval ${EXIT}
fi

# Normal execution of script using make with two Makefiles. The first Makefile
# is everything after __DATA__ below (that part is used as the master
# Qleverfile and by having it here, we avoid having two files). The second
# Makefile is the Qleverfile in this directory
# 
# First show the command before executing it (make -n).
for ARG in "$@"; do
  echo "Executing \"${ARG}\":"
  echo "${BLUE}"
  eval ${GET_MASTER_QLEVERFILE} \
    | make -n -f - -f ${APP_DIR}/Qleverfile "${ARG}" | grep -v "^echo"
  echo ${NORMAL}
  # Special precautions if index exists.
  # if [ "${ARG}" == "index" ]; then
  eval ${GET_MASTER_QLEVERFILE} \
    | make -f - -f ${APP_DIR}/Qleverfile "${ARG}"
  echo
done

eval ${EXIT}

# Everything after __DATA__ is the master Qleverfile

__DATA__
SHELL=/bin/bash
.DEFAULT_GOAL=show-config

# PIN WARMUP QUERIES TO CACHE (for the QLever UI)
# (c) Algorithms and Data Structures, University of Freiburg
# Originally written by Hannah Bast, 20.02.2021

# This Makefile provides the following targets:
#
# pin: Pin queries to cache, so that all autocompletion queries are fast, even
#      when "Clear cache" is clicked in the QLever UI (ther results for pinned 
#      queries will never be removed, unless ... see target clear).
#
# clear: Clear the cache completely (including pinned results). Note that this
#        can NOT be activated from the QLever UI.
#
# clear-unpinned: Clear all unpinned results from the cache. This is exactly
#                 what happens when clicking "Clear cache" in the QLever UI.
#
# show-all-ac-queries: Show the AC queries for subject, predicat, object for
#                      copy&paste in the QLever UI backend settings.

# This Makefile should be used as follows:
#
# 1. In the directors with the particular index, create a new Makefile
# 2. At the top add: include /local/data/qlever/qlever-indices/Makefile
#    (or wherever this Makefile - the master Makefile - resides)
# 3. Redefine API and FREQUENT_PREDICATES (see below) in the local Makefile
# 4. Redefine any of the patterns in the local Makefile
#    (the patterns below give a default functionality, which should work
#    for any knowledge base, but only using the raw IRIs, and no names,
#    aliases, or whatever special data the knowledge base has to offer.

# A prefix that identifies a particular build. This typically consists of a base
# name and optionally further specificataion separated by dots. For example:
# wikidata. Or: wikidata.2021-06-27
DB=

# The base name of the prefix=the part before the first dot. Often, the name
# of the input (ttl) file or of the settings (json) file use only the basename
# and not the full prefix.
DB_BASE=$(firstword $(subst ., ,$(DB)))

# The port of the QLever backend.
PORT=

# The slug used in the URL of the QLever API and the QleverUI API. This is
# typically the basename of the prefix. For example: wikidata. Or: freebase.
SLUG=$(DB_BASE)

# Memory for queries and for the cache (all in GB).
MEMORY_FOR_QUERIES=30
CACHE_MAX_SIZE_GB=30
CACHE_MAX_SIZE_GB_SINGLE_ENTRY=5
CACHE_MAX_NUM_ENTRIES=1000

# The URL of the QLever backend.
QLEVER_API=https://qlever.cs.uni-freiburg.de/api/$(SLUG)

# The URL of the QLever UI istance ... TODO: it's confusing that this also has
# /api/ in the name, it actually has nothing to do with the URLs from the QLever
# backends (which are defined in the Apache configuration of QLever).
WARMUP_API=$(subst /api/,/api/warmup/,$(QLEVER_API))

# Admin token
TOKEN=aof4Ad

# Frequent predicates that should be pinned to the cache (can be left empty).
# Separate by space. You can use all the prefixes from PREFIXES (e.g. wdt:P31 if
# PREFIXES defines the prefix for wdt), but you can also write full IRIs. Just
# see how it is used in target pin: below, it's very simple.
FREQUENT_PREDICATES =
FREQUENT_PATTERNS_WITHOUT_ORDER =

# Directory with the QLever binaries.
QLEVER_BIN_DIR=/local/data/qlever/qlever-code/build

# Docker-related variables.
USE_DOCKER =
DOCKER_IMAGE=adfreiburg/qlever:latest
DOCKER_CONTAINER=qlever.$(DB)

# Configuration for SPARQL+Text.
QLEVER_TOOL_DIR=$(dir $(lastword $(MAKEFILE_LIST)))misc
WITH_TEXT =
TEXT_OPTIONS_INDEX=$(if $(WITH_TEXT),-w $(DB).wordsfile.tsv -d $(DB).docsfile.tsv,)
TEXT_OPTIONS_START=$(if $(WITH_TEXT),-t,)

show-config-default:
	@ echo "Basic configuration variables:"
	@ echo -e "\033[34m"
	@ for VAR in DB SLUG CAT_TTL PORT \
	   MEMORY_FOR_QUERIES CACHE_MAX_SIZE_GB CACHE_MAX_SIZE_GB_SINGLE_ENTRY CACHE_MAX_NUM_ENTRIES \
	   TEXT_OPTIONS_INDEX TEXT_OPTIONS_START \
	   QLEVER_BIN_DIR USE_DOCKER DOCKER_IMAGE DOCKER_CONTAINER; do \
	   printf "%-30s = %s\n" "$$VAR" "$${!VAR}"; done
	@ echo -e "\033[0m"
	@ # echo "make index will do the following (but NOT if an index with that name exists):"
	@ # echo
	@ # $(MAKE) -sn index | egrep -v "^(if|fi)"
	@ # echo

%: %-default
	@ true

# Pull new image if image name contains a /

docker_pull:
	@ if [[ "$(DOCKER_IMAGE)" == */* ]]; then docker pull $(DOCKER_IMAGE); fi
	@ docker images -f "reference=$(DOCKER_IMAGE)"

# Building an index.
CAT_TTL=cat $(DB).ttl
QLEVER_INDEX_CMD=$(if $(USE_DOCKER),IndexBuilderMain,$(QLEVER_BIN_DIR)/IndexBuilderMain)
INDEX_CMD_LINE=$(CAT_TTL) | $(QLEVER_INDEX_CMD) -F ttl -f - -l -i $(DB) $(TEXT_OPTIONS_INDEX) -s $(DB_BASE).settings.json | tee $(DB).index-log.txt
INDEX_CMD_NATIVE=$(INDEX_CMD_LINE)
INDEX_CMD_DOCKER=docker run -it --rm -v $(shell pwd):/index --entrypoint bash --name qlever.$(DB)-index $(DOCKER_IMAGE) -c "cd /index && $(INDEX_CMD_LINE)"
INDEX_CMD=time ( $(if $(USE_DOCKER),$(INDEX_CMD_DOCKER),$(INDEX_CMD_NATIVE)) )
index:
	@if ls $(DB).index.* 1> /dev/null 2>&1; then echo -e "\033[31mIndex exists, delete it first with \"qlever remove_index\" (add \"show\" to find out what gets deleted)\033[0m"; else $(INDEX_CMD); fi

# Check if index exists.
# index_exists:
# 	@ if ls $(DB).index.* 1> /dev/null 2>&1; then (exit 42); else (exit 0); fi

# Starting the server in the background.
QLEVER_START_CMD=$(if $(USE_DOCKER),ServerMain,$(QLEVER_BIN_DIR)/ServerMain)
START_CMD_LINE=$(QLEVER_START_CMD) -i $(DB) -j 8 -m $(MEMORY_FOR_QUERIES) -c $(CACHE_MAX_SIZE_GB) -e $(CACHE_MAX_SIZE_GB_SINGLE_ENTRY) -k $(CACHE_MAX_NUM_ENTRIES) -p $(PORT) $(TEXT_OPTIONS_START)
START_CMD_NATIVE=$(START_CMD_LINE) > $(DB).server-log.txt & echo $$! >> $(DB).pid
START_CMD_DOCKER=docker run -d --restart=unless-stopped -p $(PORT):$(PORT) -v $(shell pwd):/index --entrypoint bash --name $(DOCKER_CONTAINER) $(DOCKER_IMAGE) -c "cd /index && $(START_CMD_LINE)"
START_CMD=$(if $(USE_DOCKER),docker rm -f $(DOCKER_CONTAINER); $(START_CMD_DOCKER),$(START_CMD_NATIVE))
start:
	@ echo "Starting the QLever server in the background ..."
	@ $(START_CMD)

# Command line for showing the log and following it.
LOG_CMD_NATIVE=tail -f -n 100 $(DB).server-log.txt
LOG_CMD_DOCKER=docker logs -f --tail 100 $(DOCKER_CONTAINER)
LOG_CMD=$(if $(USE_DOCKER),$(LOG_CMD_DOCKER),$(LOG_CMD_NATIVE))
log:
	@ echo "Showing the log (abort with Ctrl+C) ..."
	@ echo
	@ $(LOG_CMD)

# Command line for stopping the server.
STOP_CMD_NATIVE=$$(while read PID; do kill $$PID 2> /dev/null; sed -i "/^$$PID$$/d" $(DB).pid; done < $(DB).pid)
STOP_CMD_DOCKER=docker stop $(DOCKER_CONTAINER)
STOP_CMD=$(if $(USE_DOCKER),$(STOP_CMD_DOCKER),$(STOP_CMD_NATIVE))
stop:
	@ echo "Killing the QLever server (find and kill manually if it fails) ..."
	@ $(STOP_CMD)




index.OLD:
	time ( docker run -it --rm -v $(shell pwd):/index --entrypoint bash --name qlever.$(DB)-index $(DOCKER_IMAGE) -c "cd /index && $(CAT_TTL) | IndexBuilderMain -F ttl -f - -l -i $(DB) -K $(DB) $(TEXT_OPTIONS_INDEX) -s $(DB_BASE).settings.json | tee $(DB).index-log.txt" ) \

remove_index:
	@ rm -f $(DB).index.* $(DB).literals-index $(DB).vocabulary $(DB).prefixes $(DB).meta-data.json $(DB).index-log.txt
	@ echo "The index files have been removed."

# Create wordsfile and docsfile from all literals of the given NT file.
# Using this as input for a SPARQL+Text index build will effectively enable
# keyword search in literals. To understand how, look at the wordsfile and
# docsfile produced. See git:ad-freiburg/qlever/docs/sparql_and_text.md .
text_input_from_nt_literals:
	python3 $(QLEVER_TOOL_DIR)/words-and-docs-file-from-nt.py $(DB)

# START, WAIT (until the backend is read to respond), STOP, and view LOG

start.OLD:
	docker run -d --restart=unless-stopped -v $(shell pwd):/index -p $(PORT):7001 -e INDEX_PREFIX=$(DB) -e MEMORY_FOR_QUERIES=$(MEMORY_FOR_QUERIES) -e CACHE_MAX_SIZE_GB=${CACHE_MAX_SIZE_GB} -e CACHE_MAX_SIZE_GB_SINGLE_ENTRY=${CACHE_MAX_SIZE_GB_SINGLE_ENTRY} -e CACHE_MAX_NUM_ENTRIES=${CACHE_MAX_NUM_ENTRIES} --name $(DOCKER_CONTAINER) $(DOCKER_IMAGE) $(TEXT_OPTIONS_START)
	
wait:
	@ $(LOG_CMD) & PID=$$!; \
	 while [ $$(curl --silent http://localhost:$(PORT) > /dev/null; echo $$?) != 0 ]; \
	   do sleep 1; done; kill $$PID

start_and_pin:
	$(MAKE) -s start wait pin



# WARMUP queries. The .local target only works on the machine, where the
# qlever-ui Docker container is running. It has the advantage of being more
# interactive than the WARMUP_API call (which for pin: returns only after all
# warmup queries have been executed and times out if this takes too long, for
# reasons I have not fully understood yet, apparently there is a time out in one
# of the proxies involved).
HTML2ANSI=jq -r '.log|join("\n")' | sed 's|<strong>\(.*\)</strong>|\\033[1m\1\\033[0m|; s|<span style="color: blue">\(.*\)</span>|\\033[34m\1\\033[0m|' | xargs -0 echo -e

# NOTE: We need -tt here because the "qlever" script calls "make" via a pipe and
# so it does not have a terminal (the error message is "Pseudo-terminal will not
# be allocated because stdin is not a terminal." and "the input device is not a
# TTY"). The manpage for ssh says: "Multiple -t options force tty allocation,
# even if ssh has no local tty." and "-tt" is just a shorthand for "-t -t".
pin:
	@ ssh -tt galera docker exec -it qlever-ui bash -c \"python manage.py warmup $(SLUG) pin\"

pin.local:
	docker exec -it qlever-ui bash -c "python manage.py warmup $(SLUG) pin"

clear.local:
	docker exec -it qlever-ui bash -c "python manage.py warmup $(SLUG) clear"

clear_unpinned.local:
	docker exec -it qlever-ui bash -c "python manage.py warmup $(SLUG) clear_unpinned"

pin.VIA_CURL:
	@if ! curl -Gsf $(WARMUP_API)/pin?token=$(TOKEN) | $(HTML2ANSI); \
	  then curl -Gi $(WARMUP_API)/pin?token=$(TOKEN); fi

clear:
	@curl -Gs $(WARMUP_API)/clear?token=$(TOKEN) | $(HTML2ANSI)
	@# curl -Gs $(QLEVER_API) --data-urlencode "cmd=clearcachecomplete" > /dev/null

clear_unpinned:
	@curl -Gsf $(WARMUP_API)/clear_unpinned?token=$(TOKEN) | $(HTML2ANSI)
	@# curl -Gs $(QLEVER_API) --data-urlencode "cmd=clearcache" > /dev/null


# STATISTICS on cache, memory, and the number of triples per predicate.

disk_usage:
	du -hc $(DB).index.* $(DB).vocabulary.* $(DB).prefixes $(DB).meta-data.json

cachestats:
	@curl -Gs $(QLEVER_API) --data-urlencode "cmd=cachestats" \
	  | sed 's/[{}]//g; s/:/: /g; s/,/ , /g' | numfmt --field=2,5,8,11,14 --grouping && echo

memory_usage:
	@echo && docker stats --no-stream --format \
	  "Memory usage of docker container $(DOCKER_CONTAINER): {{.MemUsage}}" $(DOCKER_CONTAINER)


num_triples:
	@echo -e "\033[1mCompute total number of triples by computing the number of triples for each predicate\033[0m"
	curl -Gs $(QLEVER_API) --data-urlencode "query=SELECT ?p (COUNT(?p) AS ?count) WHERE { ?x ql:has-predicate ?p } GROUP BY ?p ORDER BY DESC(?count)" --data-urlencode "action=tsv_export" \
	  | cut -f1 | grep -v "QLever-internal-function" \
	  > $(DB).predicates.txt
	cat $(DB).predicates.txt \
	  | while read P; do \
	      $(MAKE) -s clear-unpinned > /dev/null; \
	      printf "$$P\t" && curl -Gs $(QLEVER_API) --data-urlencode "query=SELECT ?x ?y WHERE { ?x $$P ?y }" --data-urlencode "send=10" \
	        | grep resultsize | sed 's/[^0-9]//g'; \
	    done \
	  | tee $(DB).predicate-counts.tsv | numfmt --field=2 --grouping
	cut -f2 $(DB).predicate-counts.tsv | paste -sd+ | bc | numfmt --grouping \
	  | tee $(DB).num-triples.txt


# SETTINGS
settings:
	@curl -Gs $(QLEVER_API) --data-urlencode "cmd=get-settings" \
	  | sed 's/[{}]//g; s/:/: /g; s/,/ , /g' && echo

BB_FACTOR_SORTED=100
BB_FACTOR_UNSORTED=150
set_bb:
	@echo -e "\033[1mSet factor for BB FILTER cost estimate to $(BB_FACTOR)\033[0m"
	@curl -Gs $(QLEVER_API) \
	  --data-urlencode "bounding_box_filter_sorted_cost_estimate=$(BB_FACTOR_SORTED)" \
	  --data-urlencode "bounding_box_filter_unsorted_cost_estimate=$(BB_FACTOR_UNSORTED)" \
	  > \dev\null
	@$(MAKE) -s settings
	
export

